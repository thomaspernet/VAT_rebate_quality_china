{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data catalog and pre analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "import sidetable\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "\n",
    "\n",
    "name_credential = 'XXX.csv'\n",
    "region = ''\n",
    "bucket = ''\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)\n",
    "glue = service_glue.connect_glue(client = client,\n",
    "                      bucket = bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documente and analyse data\n",
    "\n",
    "In the first part of the notebook, we will create the data catalog and put the data in the `README.md`. In the second part of the notebook, we will analyse the data. Data analysis contains categorical and continuous variables. It is a batch analysis, nothing should done.\n",
    "\n",
    "# Download data locally\n",
    "\n",
    "First of all, load the data locally. Use the function `list_all_files_with_prefix` to parse all the files in a given folder. Change the prefix to the name of the folder in which the data are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'DATA/RAW_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = False\n",
    "if to_download:\n",
    "LOCAL_PATH_CONFIG_FILE = os.getcwd()\n",
    "FILES_TO_UPLOAD = s3.list_all_files_with_prefix(prefix=prefix)\n",
    "list(\n",
    "    map(\n",
    "        lambda x:\n",
    "        s3.download_file(key=x, path_local=LOCAL_PATH_CONFIG_FILE),\n",
    "        FILES_TO_UPLOAD\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data catalog\n",
    "\n",
    "The data catalogue is a json file that we save in the folder `schema`. The schema is the following:\n",
    "\n",
    "```\n",
    "{\n",
    "        \"Table\": {\"Name\": \"\", \"StorageDescriptor\": {\"Columns\": [], \"Location\": \"\"}}\n",
    "    }\n",
    "``` \n",
    "\n",
    "The schema is automatically detected and generated from `FILES_TO_UPLOAD`. Since we don't know in advance the field, we cannot add comments at first. To add comments, please refer to the next part. \n",
    "\n",
    "### Create and save data catalog\n",
    "\n",
    "The schemas are saved locally in `schema/FILENAME`. Push the schema to GitHub for availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_schema(filename):\n",
    "    \"\"\"\n",
    "    Prepare a json which is similar to glue schema.\n",
    "    It includes table name, columns, and path to S3\n",
    "\n",
    "    Output saved in schema/FILENAME\n",
    "    ARGS:\n",
    "\n",
    "    filename: string. filename of the doc to get the schema\n",
    "    \"\"\"\n",
    "\n",
    "    schema_ = {\n",
    "        \"Table\": {\"Name\": \"\", \"StorageDescriptor\": {\"Columns\": [], \"Location\": {'s3URI':\"\", 's3Bucket': ''}}}\n",
    "    }\n",
    "\n",
    "    temp = pd.read_excel(filename)\n",
    "    schema = pd.io.json.build_table_schema(temp)\n",
    "    schema_[\"Table\"][\"Name\"] = filename\n",
    "    schema_[\"Table\"][\"StorageDescriptor\"][\"Location\"]['s3URI'] = os.path.join(\n",
    "        \"s3://\", bucket, prefix, filename\n",
    "    )\n",
    "    schema_[\"Table\"][\"StorageDescriptor\"][\"Location\"]['s3Bucket'] = os.path.join(\n",
    "        \"https://s3.console.aws.amazon.com/s3\", bucket, prefix, filename\n",
    "    )\n",
    "    for i, name in enumerate(schema[\"fields\"]):\n",
    "        col = {\"Name\": name[\"name\"], \"Type\": name[\"type\"], \"Comment\": \"\"}\n",
    "        schema_[\"Table\"][\"StorageDescriptor\"][\"Columns\"].append(col)\n",
    "\n",
    "    path_name = os.path.join(\"schema\", filename)\n",
    "    with open(\"{}.json\".format(path_name), \"w\") as outfile:\n",
    "        json.dump(schema_, outfile)\n",
    "        \n",
    "    return schema_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in enumerate(FILES_TO_UPLOAD):\n",
    "    table = os.path.split(value)[1]\n",
    "    schema = prepare_schema(table)\n",
    "    print(json.dumps(schema, indent=4, sort_keys=False, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add comment\n",
    "\n",
    "This part is optional but strongly recommended. In this part, you are free to add any comment you need. To add a comment, alter the metadata of the file you want. To modify the comment, please, use:\n",
    "\n",
    "```\n",
    "[\n",
    "   {\n",
    "      \"Name\":\"\",\n",
    "      \"Type\":\"\",\n",
    "      \"Comment\":\"\"\n",
    "   }\n",
    "]\n",
    "```\n",
    "\n",
    "Fill only the variables you need to alter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_schema_table(filename, schema):\n",
    "    \"\"\"\n",
    "    database: Database name\n",
    "        table: Table name\n",
    "        schema: a list of dict:\n",
    "        [\n",
    "        {\n",
    "        'Name': 'geocode4_corr',\n",
    "        'Type': '',\n",
    "        'Comment': 'Official chinese city ID'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "    with open('{}.json'.format(os.path.join(\"schema\", filename)), 'r') as fp:\n",
    "        parameters = json.load(fp)\n",
    "        \n",
    "    list_schema = parameters['Table']['StorageDescriptor']['Columns']\n",
    "    for field in list_schema:\n",
    "        try:\n",
    "            field['Comment'] = next(\n",
    "                    item for item in schema if item[\"Name\"] == field['Name']\n",
    "                )['Comment']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    parameters['Table']['StorageDescriptor']['Columns'] = list_schema\n",
    "    path_name = os.path.join(\"schema\", filename)\n",
    "    with open(\"{}.json\".format(path_name), \"w\") as outfile:\n",
    "        json.dump(parameters, outfile)\n",
    "        \n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_alter = ''\n",
    "new_schema = [\n",
    "   {\n",
    "      \"Name\":\"\",\n",
    "      \"Type\":\"\",\n",
    "      \"Comment\":\" \"\n",
    "   }\n",
    "]\n",
    "update_schema_table(filename = filename_to_alter, schema = new_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate README \n",
    "\n",
    "The README is generated from `FILES_TO_UPLOAD` and will parse all the schema is `schema/FILENAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "README = \"# Data Catalogue\"\n",
    "for key, value in enumerate(FILES_TO_UPLOAD):\n",
    "    filename = os.path.split(value)[1]\n",
    "    with open('{}.json'.format(os.path.join(\"schema\", filename)), 'r') as fp:\n",
    "        parameters = json.load(fp)\n",
    "    tb = pd.json_normalize(parameters['Table']['StorageDescriptor']['Columns']).to_markdown()\n",
    "    template = \"\"\"\n",
    "\n",
    "## Table {0}\n",
    "\n",
    "- Filename: {1}\n",
    "- Location: {2}\n",
    "- S3uri: `{3}`\n",
    "\n",
    "\n",
    "{4}\n",
    "\n",
    "\"\"\"\n",
    "    filename_no_extension = os.path.splitext(filename)[0]\n",
    "    filename_extension = parameters['Table']['Name']\n",
    "    location = parameters['Table']['StorageDescriptor']['Location']['s3Bucket']\n",
    "    uri = parameters['Table']['StorageDescriptor']['Location']['s3URI']\n",
    "    \n",
    "    README += template.format(filename_no_extension, filename_extension,location, uri, tb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"README.md\", \"w\") as outfile:\n",
    "    outfile.write(README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "The notebook file already contains code to analyse the dataset. It contains codes to count the number of observations for a given variables, for a group and a pair of group. It also has queries to provide the distribution for a single column, for a group and a pair of group. The queries are available in the key `ANALYSIS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "**Count obs by group**\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILENAME 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(os.path.split(FILES_TO_UPLOAD[0])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the values fior each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {'var': [],\n",
    "       'count':[],\n",
    "       'values': []}\n",
    "for v in df_test.select_dtypes(include='object').columns:\n",
    "    cat = df_test[v].nunique()\n",
    "    value_cat  = df_test[v].unique()\n",
    "    dic_['var'].append(v)\n",
    "    dic_['count'].append(cat)\n",
    "    dic_['values'].append(value_cat)\n",
    "(pd.DataFrame(dic_)\n",
    " .sort_values(by = ['count'], ascending = False)\n",
    " .set_index('var')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    df_count = df_test.stb.freq([objects])\n",
    "    if df_count.shape[0] > 20:\n",
    "        df_count = df_count.iloc[:20, :]\n",
    "    display(\n",
    "        (\n",
    "            df_count.reset_index(drop=True)\n",
    "            .style.format(\n",
    "                \"{0:,.2%}\", subset=[\"Percent\", \"Cumulative Percent\"], na_rep=\"-\"\n",
    "            )\n",
    "            .bar(subset=[\"Cumulative Percent\"], color=\"#d65f5f\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    if objects not in [primary_key]:\n",
    "        df_count = df_test.stb.freq([objects])\n",
    "        if df_count.shape[0] > 20:\n",
    "            df_count = df_count.iloc[:20, :]\n",
    "        display(\n",
    "            (\n",
    "                df_test.stb.freq([primary_key, objects])\n",
    "                .set_index([primary_key, objects])\n",
    "                .drop(columns=['Cumulative Count', 'Cumulative Percent'])\n",
    "                .iloc[:20, :]\n",
    "                .unstack(-1)\n",
    "                .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", subset=[\"Percent\"], na_rep=\"-\"\n",
    "                )\n",
    "                .format(\n",
    "                    \"{0:,.2f}\", subset=[\"Count\"], na_rep=\"-\"\n",
    "                )\n",
    "                .background_gradient(\n",
    "                    cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"Count\")\n",
    "                )\n",
    "\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "- Display the percentile\n",
    "- Display the percentile, with one primary key\n",
    "- Display the percentile, with one primary key, and a secondary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_alter = \"\"\n",
    "df_test = pd.read_excel(filename_to_alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_test\n",
    "    .describe()\n",
    "    .style.format(\"{0:.2f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\", 'boolean']).columns)[:1]:\n",
    "    if objects not in [primary_key]:\n",
    "        display(\n",
    "            (\n",
    "                df_test\n",
    "                .groupby(primary_key)\n",
    "                .describe()[objects]\n",
    "                .sort_values(by='count', ascending=False)\n",
    "                .iloc[:20, :]\n",
    "                .style.format(\"{0:.2f}\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.23.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
