{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Poc compute export share covariates by firms ownsership\n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "- In the context of a poc,  add covariates related to the export profil of a given city. four covariates can be added to the model:\n",
    "    * Lag foreign export share by city, product, destination, regime\n",
    "    * Lag foreign export share by city, product, regime \n",
    "    * Lag SOE export share by city, product, destination, regime\n",
    "    * Lag SOE export share by city, product, regime\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Epic: Epic 1\n",
    "* US: US 3\n",
    "* Date Begin: 10/3/2020\n",
    "* Duration Task: 0\n",
    "* Description: Add foreign and SOE export shares variables \n",
    "* Step type:  \n",
    "* Status: Active\n",
    "* Source URL: US 03 Export share\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://468786073381.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 8\n",
    "* Task tag: #data-preparation,#athena,#sql\n",
    "* Toggl Tag: #data-preparation\n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name: \n",
    "* import_export\n",
    "* Github: \n",
    "  * https://github.com/thomaspernet/VAT_rebate_quality_china/blob/master/01_data_preprocessing/01_prepare_tables/01_tables_trade_tariffs_taxes.md\n",
    "  \n",
    "# Destination Output/Delivery  \n",
    "  \n",
    "## Table/file\n",
    "* Origin: \n",
    "* Athena\n",
    "* Name:\n",
    "* lag_foreign_export_ckjr\n",
    "* lag_foreign_export_ckr\n",
    "* lag_soe_export_ckjr\n",
    "* lag_soe_export_ckr\n",
    "* GitHub:\n",
    "* https://github.com/thomaspernet/VAT_rebate_quality_china/blob/master/01_data_preprocessing/02_prepare_tables_model/00_POC_prepare_tables_model/00_export_share_foreign_SOE.md\n",
    "\n",
    "# Knowledge\n",
    "\n",
    "## List of candidates\n",
    "\n",
    "* Trade policy repercussions: the role of local product space-Evidence from China\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "\n",
    "\n",
    "name_credential = 'thomas_vat_credentials.csv'\n",
    "region = 'eu-west-3'\n",
    "bucket = 'chinese-data'\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = True) \n",
    "glue = service_glue.connect_glue(client = client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare query POC\n",
    "\n",
    "This notebook is in a POC stage, which means, you will write your queries and tests if it works. Once you are satisfied by the jobs, move the queries to the ETL. Prepare a pseudo JSON file to spare time during the US linked to the ETL. Use the following format to validate the queries:\n",
    "\n",
    "## Templare prepare table\n",
    "\n",
    "- To create a new table using existing table (i.e Athena tables), copy the template below and paste it inside the list `TABLES.PREPARATION.ALL_SCHEMA`\n",
    "    - The list `ALL_SCHEMA` accepts one or more steps. Each steps, `STEPS_X` can be a sequence of queries execution. \n",
    "\n",
    "```\n",
    "\"PREPARATION\":{\n",
    "   \"ALL_SCHEMA\":[\n",
    "      {\n",
    "         \"STEPS_0\":{\n",
    "            \"name\":\"\",\n",
    "            \"execution\":[\n",
    "               {\n",
    "                  \"database\":\"\",\n",
    "                  \"name\":\"\",\n",
    "                  \"output_id\":\"\",\n",
    "                  \"query\":{\n",
    "                     \"top\":\"\",\n",
    "                     \"middle\":\"\",\n",
    "                     \"bottom\":\"\"\n",
    "                  }\n",
    "               }\n",
    "            ],\n",
    "            \"schema\":[\n",
    "               {\n",
    "                  \"Name\":\"\",\n",
    "                  \"Type\":\"\",\n",
    "                  \"Comment\":\"\"\n",
    "               }\n",
    "            ]\n",
    "         }\n",
    "      }\n",
    "   ],\n",
    "   \"template\":{\n",
    "      \"top\":\"CREATE TABLE {}.{} WITH (format = 'PARQUET') AS \"\n",
    "   }\n",
    "}\n",
    "``` \n",
    "\n",
    "To add a step, use this template inside `TABLES.PREPARATION.ALL_SCHEMA`\n",
    "\n",
    "```\n",
    "{\n",
    "   \"STEPS_X\":{\n",
    "      \"name\":\"\",\n",
    "      \"execution\":[\n",
    "         {\n",
    "            \"database\":\"\",\n",
    "            \"name\":\"\",\n",
    "            \"output_id\":\"\",\n",
    "            \"query\":{\n",
    "               \"top\":\"\",\n",
    "               \"middle\":\"\",\n",
    "               \"bottom\":\"\"\n",
    "            }\n",
    "         }\n",
    "      ],\n",
    "      \"schema\":[\n",
    "               {\n",
    "                  \"Name\":\"\",\n",
    "                  \"Type\":\"\",\n",
    "                  \"Comment\":\"\"\n",
    "               }\n",
    "            ]\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "To add a query execution with a within, use the following template inside the list `STEPS_X.execution`\n",
    "\n",
    "```\n",
    "{\n",
    "   \"database\":\"\",\n",
    "   \"name\":\"\",\n",
    "   \"output_id\":\"\",\n",
    "   \"query\":{\n",
    "      \"top\":\"\",\n",
    "      \"middle\":\"\",\n",
    "      \"bottom\":\"\"\n",
    "   }\n",
    "}\n",
    "``` \n",
    "\n",
    "\n",
    "\n",
    "Each step name should follow this format `STEPS_0`, `STEPS_1`, `STEPS_2`, etc\n",
    "\n",
    "## Templare add comments to Glue\n",
    "\n",
    "The AWS Glue Data Catalog contains references to data that is used as sources and targets of your extract, transform, and load (ETL) jobs in AWS Glue. To create your data warehouse or data lake, you must catalog this data. The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. You use the information in the Data Catalog to create and monitor your ETL jobs. Information in the Data Catalog is stored as metadata tables, where each table specifies a single data store.\n",
    "\n",
    "We make use of the `boto3` API to add comments in the metastore. \n",
    "\n",
    "- To alter the metadata (only comments), copy the template below and paste inside the list `PREPARATION.STEPS_X.schema`. \n",
    "\n",
    "```\n",
    "[\n",
    "   {\n",
    "      \"Name\":\"\",\n",
    "      \"Type\":\"\",\n",
    "      \"Comment\":\"\"\n",
    "   }\n",
    "]\n",
    "```\n",
    "\n",
    "The schema is related to a table, and will be modified by Glue API. **Only** variables inside the list will be modified, the remaining variables will keep default's value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "- Lag Export growth by city, product, destination\n",
    "- Lag Export growth by city, product\n",
    "- Lag foreign export share by city, product, destination, regime\n",
    "- Lag foreign export share by city, product, regime\n",
    "- Lag SOE export share by city, product, destination, regime\n",
    "- Lag SOE export share by city, product, regime\n",
    "- Merge all tables\n",
    "\n",
    "From [Trade policy repercussions: the role of local product space-Evidence from China](https://docs.google.com/file/d/1n8ociwqDVBaUKRTOmFX1mJepwIYqXlbd/edit)\n",
    "\n",
    "2 Need to keep this variable Business_type  to compute the share, and the business types we are interested in are\n",
    "  * SOE: 国有企业 \n",
    "  * Foreign: 外商独资企业 \n",
    "  \n",
    "## Construction and data sources of control variables\n",
    "\n",
    "Since it is still possible that local export dynamics for a given product vary by trade\n",
    "regime or city, we add a vector of control variables $X_{c,k,j, t-1}^{R}$, with coefficient vector $\\lambda$. Therefore, we include the share of exports by foreign firms ($\\text{Foreign export share}_{c,k,j, t-1}^{R}$) and the share of state-owned firms ($\\text{State export share}_{c,k,j, t-1}^{R}$) defined at the city-product-regime level. \n",
    "\n",
    "These two controls are crucial to account for the time-varying ability of different localities to export\n",
    "different products (under different regimes) as export performance in China varies greatly by\n",
    "firm ownership (Amiti and Freund, 2010). We further include the change in city-product\n",
    "export values from t-2 to t-1 ($\\text{Export growth}_{c,k,j, t-1}$) to account for export dynamics at the\n",
    "city and HS6 product level.\n",
    "\n",
    "The Customs trade data is used to obtain several of our control variables: $\\text{Export growth}_{k,j,t-1}$,\n",
    "$\\text{Export growth}_{c,k,j, t-1}$, $\\text{Foreign export share}_{c,k,j, t-1}^{R}$\n",
    "and $\\text{State export share}_{c,k,j, t-1}^{R}$.\n",
    "\n",
    "$\\text{Export growth}_{k,j,t-1}$ and $\\text{Export growth}_{c,k,j, t-1}$ are yearly export growth at the product-level\n",
    "and at the city-product level respectively. These proxies of export dynamics are computed\n",
    "using the mid-point growth rate formula using export values from t-2 and t-1. $\\text{Foreign export share}_{c,k,j, t-1}^{R}$\n",
    "and $\\text{State export share}_{c,k,j, t-1}^{R}$ measure respectively the share of export quantities by foreign and state-owned firms for each product-city-regime combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Methodolology\n",
    "\n",
    "For each query below, we store them in a dataset named `temporary` in Athena, then we will merge them with `quality_vat_export_2003_2010`.\n",
    "\n",
    "We keep the exact same number of observations (5,836,151 in `quality_vat_export_2003_2010`, and replace `NULL`with 0.\n",
    "\n",
    "The new tables are the following:\n",
    "\n",
    "* `lag_foreign_export_ckjr`\n",
    "* `lag_foreign_export_ckr`\n",
    "* `lag_soe_export_ckjr`\n",
    "* `lag_soe_export_ckr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(key = 'DATA/ETL/parameters_ETL.json')\n",
    "with open('parameters_ETL.json', 'r') as fp:\n",
    "    parameters = json.load(fp)\n",
    "db = parameters['GLOBAL']['DATABASE']\n",
    "s3_output = parameters['GLOBAL']['QUERIES_OUTPUT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime('%Y%M%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag foreign export share by city, product, destination, regime\n",
    "\n",
    "- Table name: `lag_foreign_export_ckjr`\n",
    "\n",
    "Some countries like SER have more than one chinese name so we filter only one country amonf the duplicates. Easiest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM chinese_lookup.country_cn_en \n",
    "WHERE iso_alpha =  'SER'\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM \"chinese_lookup\".\"country_cn_en\" \n",
    "WHERE iso_alpha = 'FRA' OR\n",
    "iso_alpha = \t'UAE'  OR \n",
    "iso_alpha = \t'ANT' OR \n",
    "iso_alpha = \t'BMU' OR \n",
    "iso_alpha = \t'MTQ' OR\n",
    "iso_alpha = \t'DMA' OR\n",
    "iso_alpha = \t'ABW' OR\n",
    "iso_alpha = \t'GLP'\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE lag_foreign_export_ckjr\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE chinese_trade.lag_foreign_export_ckjr\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH filter_data AS (\n",
    "  SELECT \n",
    "    date as year, \n",
    "    id, \n",
    "    trade_type, \n",
    "    business_type, \n",
    "    CASE WHEN length(hs) < 5 THEN CONCAT('0', hs) ELSE hs END as hs6, \n",
    "    city_prod, \n",
    "    CASE \n",
    "    WHEN origin_or_destination = '阿鲁巴' OR origin_or_destination = '阿鲁巴岛' THEN '阿鲁巴岛' \n",
    "    WHEN origin_or_destination = '荷属安地列斯群岛' OR origin_or_destination = '荷属安的列斯' THEN '荷属安的列斯' \n",
    "    WHEN origin_or_destination = '百慕大' OR origin_or_destination = '百慕大群岛' THEN '百慕大群岛' \n",
    "    WHEN origin_or_destination = '多米尼克' OR origin_or_destination = '多米尼加' THEN '多米尼加' \n",
    "    WHEN origin_or_destination = '法国' OR origin_or_destination = '马约特' OR origin_or_destination = '马约特岛' THEN '法国' \n",
    "    WHEN origin_or_destination = '瓜德罗普' OR origin_or_destination = '瓜德罗普岛' THEN '瓜德罗普岛' \n",
    "    WHEN origin_or_destination = '马提尼克' OR origin_or_destination = '马提尼克岛' THEN '马提尼克岛' \n",
    "    WHEN origin_or_destination = '阿拉伯联合酋长国' OR origin_or_destination = '阿联酋' THEN '阿拉伯联合酋长国' \n",
    "    WHEN origin_or_destination = '巴哈马' OR origin_or_destination = '巴林' THEN '巴林' \n",
    "    WHEN origin_or_destination = '中国' OR origin_or_destination = '中华人民共和国' OR origin_or_destination = '台湾省' THEN '中国' \n",
    "    WHEN origin_or_destination = '南斯拉夫联盟共和国' OR origin_or_destination = '前南马其顿' OR origin_or_destination = '前南斯拉夫马其顿共和国'  THEN '南斯拉夫联盟共和国' \n",
    "    WHEN origin_or_destination = '吉尔吉斯' OR origin_or_destination = '吉尔吉斯斯坦' THEN '吉尔吉斯斯坦' \n",
    "    WHEN origin_or_destination = '黑山' OR origin_or_destination = '塞尔维亚' THEN '塞尔维亚' \n",
    "    ELSE origin_or_destination END AS destination, \n",
    "    quantities, \n",
    "    value, \n",
    "    CASE WHEN trade_type = '进料加工贸易' \n",
    "    OR trade_type = '一般贸易' THEN 'ELIGIBLE' ELSE 'NOT_ELIGIBLE' END as regime, \n",
    "    CASE WHEN business_type = '外商独资企业' THEN 'FOREIGN' ELSE 'NO_FOREIGN' END as foreign_ownership \n",
    "  FROM \n",
    "    chinese_trade.import_export \n",
    "  WHERE \n",
    "    date in (\n",
    "      '2002', '2003', '2004', '2005', '2006', \n",
    "      '2007', '2008', '2009', '2010'\n",
    "    ) \n",
    "    AND imp_exp = '出口' \n",
    "    AND (\n",
    "      trade_type = '进料加工贸易' \n",
    "      OR trade_type = '一般贸易' \n",
    "      OR trade_type = '来料加工装配贸易' \n",
    "      OR trade_type = '加工贸易'\n",
    "    ) \n",
    "    AND intermediate = 'No' \n",
    "    AND quantities > 0 \n",
    "    AND value > 0\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH aggregate as (\n",
    "      SELECT \n",
    "        year, \n",
    "        CAST(\n",
    "          CAST(year AS INTEGER) -1 AS VARCHAR\n",
    "        ) as year_lag, \n",
    "        regime, \n",
    "        foreign_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        destination, \n",
    "        SUM(quantities) as quantities \n",
    "      FROM \n",
    "        filter_data \n",
    "      GROUP BY \n",
    "        year, \n",
    "        regime, \n",
    "        foreign_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        destination\n",
    "    ) \n",
    "    SELECT \n",
    "      aggregate.year, \n",
    "      aggregate.year_lag, \n",
    "      aggregate.regime, \n",
    "      aggregate.foreign_ownership, \n",
    "      geocode4_corr,\n",
    "      iso_alpha,\n",
    "      aggregate.HS6, \n",
    "      quantities, \n",
    "      CASE WHEN quantities_lag IS NULL THEN 0 ELSE quantities_lag END AS quantities_lag, \n",
    "      CASE WHEN total_quantities_lag IS NULL THEN 0 ELSE total_quantities_lag END AS total_quantities_lag, \n",
    "      CASE WHEN quantities_lag IS NULL \n",
    "      OR total_quantities_lag IS NULL THEN 0 ELSE CAST(\n",
    "        quantities_lag AS DECIMAL(16, 5)\n",
    "      )/ CAST(\n",
    "        total_quantities_lag AS DECIMAL(16, 5)\n",
    "      ) END AS lag_foreign_export_share_ckjr \n",
    "    FROM \n",
    "      aggregate \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          foreign_ownership, \n",
    "          city_prod, \n",
    "          destination,\n",
    "          HS6, \n",
    "          quantities as quantities_lag \n",
    "        FROM \n",
    "          aggregate\n",
    "      ) as lag_quantities ON aggregate.year_lag = lag_quantities.year \n",
    "      AND aggregate.regime = lag_quantities.regime \n",
    "      AND aggregate.foreign_ownership = lag_quantities.foreign_ownership \n",
    "      AND aggregate.city_prod = lag_quantities.city_prod \n",
    "      AND aggregate.HS6 = lag_quantities.HS6 \n",
    "      AND aggregate.destination = lag_quantities.destination \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          destination, \n",
    "          SUM(quantities) as total_quantities_lag \n",
    "        FROM \n",
    "          aggregate \n",
    "        GROUP BY \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          destination\n",
    "      ) as group_lag ON aggregate.year_lag = group_lag.year \n",
    "      AND aggregate.regime = group_lag.regime \n",
    "      AND aggregate.city_prod = group_lag.city_prod \n",
    "      AND aggregate.HS6 = group_lag.HS6 \n",
    "      AND aggregate.destination = group_lag.destination \n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          DISTINCT(citycn) as citycn, \n",
    "          cityen, \n",
    "          geocode4_corr \n",
    "        FROM \n",
    "          chinese_lookup.city_cn_en\n",
    "      ) AS city_cn_en ON city_cn_en.citycn = aggregate.city_prod \n",
    "      LEFT JOIN chinese_lookup.country_cn_en\n",
    "      ON country_cn_en.Country_cn = aggregate.destination \n",
    "      WHERE aggregate.foreign_ownership = 'FOREIGN' AND iso_alpha IS NOT NULL\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT CNT, COUNT(*) AS CNT_DUPLICATE\n",
    "FROM (\n",
    "SELECT year_lag, regime, geocode4_corr, iso_alpha, hs6, COUNT(*) as CNT\n",
    "FROM \"chinese_trade\".\"lag_foreign_export_ckjr\" \n",
    "GROUP BY \n",
    "year_lag, regime, geocode4_corr, iso_alpha, hs6\n",
    "  )\n",
    "  GROUP BY CNT\n",
    "  ORDER BY CNT_DUPLICATE\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag foreign export share by city, product, regime\n",
    "\n",
    "Table name: `lag_foreign_export_ckr`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE lag_foreign_export_ckr\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE chinese_trade.lag_foreign_export_ckr\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH filter_data AS (\n",
    "  SELECT \n",
    "    date as year, \n",
    "    id, \n",
    "    trade_type, \n",
    "    business_type, \n",
    "    CASE WHEN length(hs) < 5 THEN CONCAT('0', hs) ELSE hs END as hs6, \n",
    "    city_prod, \n",
    "    quantities, \n",
    "    value, \n",
    "    CASE WHEN trade_type = '进料加工贸易' \n",
    "    OR trade_type = '一般贸易' THEN 'ELIGIBLE' ELSE 'NOT_ELIGIBLE' END as regime, \n",
    "    CASE WHEN business_type = '外商独资企业' THEN 'FOREIGN' ELSE 'NO_FOREIGN' END as foreign_ownership \n",
    "  FROM \n",
    "    chinese_trade.import_export \n",
    "  WHERE \n",
    "    date in (\n",
    "      '2002', '2003', '2004', '2005', '2006', \n",
    "      '2007', '2008', '2009', '2010'\n",
    "    ) \n",
    "    AND imp_exp = '出口' \n",
    "    AND (\n",
    "      trade_type = '进料加工贸易' \n",
    "      OR trade_type = '一般贸易' \n",
    "      OR trade_type = '来料加工装配贸易' \n",
    "      OR trade_type = '加工贸易'\n",
    "    ) \n",
    "    AND intermediate = 'No' \n",
    "    AND quantities > 0 \n",
    "    AND value > 0\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH aggregate as (\n",
    "      SELECT \n",
    "        year, \n",
    "        CAST(\n",
    "          CAST(year AS INTEGER) -1 AS VARCHAR\n",
    "        ) as year_lag, \n",
    "        regime, \n",
    "        foreign_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        SUM(quantities) as quantities \n",
    "      FROM \n",
    "        filter_data \n",
    "      GROUP BY \n",
    "        year, \n",
    "        regime, \n",
    "        foreign_ownership, \n",
    "        city_prod, \n",
    "        HS6\n",
    "    ) \n",
    "    SELECT \n",
    "      aggregate.year, \n",
    "      aggregate.year_lag, \n",
    "      aggregate.regime, \n",
    "      aggregate.foreign_ownership, \n",
    "      geocode4_corr,\n",
    "      aggregate.HS6, \n",
    "      quantities, \n",
    "      CASE WHEN quantities_lag IS NULL THEN 0 ELSE quantities_lag END AS quantities_lag, \n",
    "      CASE WHEN total_quantities_lag IS NULL THEN 0 ELSE total_quantities_lag END AS total_quantities_lag, \n",
    "      CASE WHEN quantities_lag IS NULL \n",
    "      OR total_quantities_lag IS NULL THEN 0 ELSE CAST(\n",
    "        quantities_lag AS DECIMAL(16, 5)\n",
    "      )/ CAST(\n",
    "        total_quantities_lag AS DECIMAL(16, 5)\n",
    "      ) END AS lag_foreign_export_share_ckr \n",
    "    FROM \n",
    "      aggregate \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          foreign_ownership, \n",
    "          city_prod, \n",
    "          HS6, \n",
    "          quantities as quantities_lag \n",
    "        FROM \n",
    "          aggregate\n",
    "      ) as lag_quantities ON aggregate.year_lag = lag_quantities.year \n",
    "      AND aggregate.regime = lag_quantities.regime \n",
    "      AND aggregate.foreign_ownership = lag_quantities.foreign_ownership \n",
    "      AND aggregate.city_prod = lag_quantities.city_prod \n",
    "      AND aggregate.HS6 = lag_quantities.HS6 \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          SUM(quantities) as total_quantities_lag \n",
    "        FROM \n",
    "          aggregate \n",
    "        GROUP BY \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod \n",
    "      ) as group_lag ON aggregate.year_lag = group_lag.year \n",
    "      AND aggregate.regime = group_lag.regime \n",
    "      AND aggregate.city_prod = group_lag.city_prod \n",
    "      AND aggregate.HS6 = group_lag.HS6\n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          DISTINCT(citycn) as citycn, \n",
    "          cityen, \n",
    "          geocode4_corr \n",
    "        FROM \n",
    "          chinese_lookup.city_cn_en\n",
    "      ) AS city_cn_en ON city_cn_en.citycn = aggregate.city_prod \n",
    "      WHERE aggregate.foreign_ownership = 'FOREIGN' \n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT CNT, COUNT(*) AS CNT_DUPLICATE\n",
    "FROM (\n",
    "SELECT year_lag, regime, geocode4_corr, hs6, COUNT(*) as CNT\n",
    "FROM \"chinese_trade\".\"lag_foreign_export_ckr\" \n",
    "GROUP BY \n",
    "year_lag, regime, geocode4_corr, hs6\n",
    "  )\n",
    "  GROUP BY CNT\n",
    "  ORDER BY CNT_DUPLICATE\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag SOE export share by city, product, destination, regime\n",
    "\n",
    "- Table name: `lag_soe_export_ckjr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE lag_soe_export_ckjr\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE chinese_trade.lag_soe_export_ckjr\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH filter_data AS (\n",
    "  SELECT \n",
    "    date as year, \n",
    "    id, \n",
    "    trade_type, \n",
    "    business_type, \n",
    "    CASE WHEN length(hs) < 5 THEN CONCAT('0', hs) ELSE hs END as hs6, \n",
    "    city_prod, \n",
    "    CASE \n",
    "    WHEN origin_or_destination = '阿鲁巴' OR origin_or_destination = '阿鲁巴岛' THEN '阿鲁巴岛' \n",
    "    WHEN origin_or_destination = '荷属安地列斯群岛' OR origin_or_destination = '荷属安的列斯' THEN '荷属安的列斯' \n",
    "    WHEN origin_or_destination = '百慕大' OR origin_or_destination = '百慕大群岛' THEN '百慕大群岛' \n",
    "    WHEN origin_or_destination = '多米尼克' OR origin_or_destination = '多米尼加' THEN '多米尼加' \n",
    "    WHEN origin_or_destination = '法国' OR origin_or_destination = '马约特' OR origin_or_destination = '马约特岛' THEN '法国' \n",
    "    WHEN origin_or_destination = '瓜德罗普' OR origin_or_destination = '瓜德罗普岛' THEN '瓜德罗普岛' \n",
    "    WHEN origin_or_destination = '马提尼克' OR origin_or_destination = '马提尼克岛' THEN '马提尼克岛' \n",
    "    WHEN origin_or_destination = '阿拉伯联合酋长国' OR origin_or_destination = '阿联酋' THEN '阿拉伯联合酋长国' \n",
    "    WHEN origin_or_destination = '巴哈马' OR origin_or_destination = '巴林' THEN '巴林' \n",
    "    WHEN origin_or_destination = '中国' OR origin_or_destination = '中华人民共和国' OR origin_or_destination = '台湾省' THEN '中国' \n",
    "    WHEN origin_or_destination = '南斯拉夫联盟共和国' OR origin_or_destination = '前南马其顿' OR origin_or_destination = '前南斯拉夫马其顿共和国'  THEN '南斯拉夫联盟共和国' \n",
    "    WHEN origin_or_destination = '吉尔吉斯' OR origin_or_destination = '吉尔吉斯斯坦' THEN '吉尔吉斯斯坦' \n",
    "    WHEN origin_or_destination = '黑山' OR origin_or_destination = '塞尔维亚' THEN '塞尔维亚' \n",
    "    ELSE origin_or_destination END AS destination,\n",
    "    quantities, \n",
    "    value, \n",
    "    CASE WHEN trade_type = '进料加工贸易' \n",
    "    OR trade_type = '一般贸易' THEN 'ELIGIBLE' ELSE 'NOT_ELIGIBLE' END as regime, \n",
    "    CASE WHEN Business_type = '国有企业' OR Business_type = '国有' THEN 'SOE' ELSE 'NO_SOE' END as SOE_ownership\n",
    "  FROM \n",
    "    chinese_trade.import_export \n",
    "  WHERE \n",
    "    date in (\n",
    "      '2002', '2003', '2004', '2005', '2006', \n",
    "      '2007', '2008', '2009', '2010'\n",
    "    ) \n",
    "    AND imp_exp = '出口' \n",
    "    AND (\n",
    "      trade_type = '进料加工贸易' \n",
    "      OR trade_type = '一般贸易' \n",
    "      OR trade_type = '来料加工装配贸易' \n",
    "      OR trade_type = '加工贸易'\n",
    "    ) \n",
    "    AND intermediate = 'No' \n",
    "    AND quantities > 0 \n",
    "    AND value > 0\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH aggregate as (\n",
    "      SELECT \n",
    "        year, \n",
    "        CAST(\n",
    "          CAST(year AS INTEGER) -1 AS VARCHAR\n",
    "        ) as year_lag, \n",
    "        regime, \n",
    "        SOE_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        destination, \n",
    "        SUM(quantities) as quantities \n",
    "      FROM \n",
    "        filter_data \n",
    "      GROUP BY \n",
    "        year, \n",
    "        regime, \n",
    "        SOE_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        destination\n",
    "    ) \n",
    "    SELECT \n",
    "      aggregate.year, \n",
    "      aggregate.year_lag, \n",
    "      aggregate.regime, \n",
    "      aggregate.SOE_ownership, \n",
    "      geocode4_corr,\n",
    "      iso_alpha,\n",
    "      aggregate.HS6, \n",
    "      quantities, \n",
    "      CASE WHEN quantities_lag IS NULL THEN 0 ELSE quantities_lag END AS quantities_lag, \n",
    "      CASE WHEN total_quantities_lag IS NULL THEN 0 ELSE total_quantities_lag END AS total_quantities_lag, \n",
    "      CASE WHEN quantities_lag IS NULL \n",
    "      OR total_quantities_lag IS NULL THEN 0 ELSE CAST(\n",
    "        quantities_lag AS DECIMAL(16, 5)\n",
    "      )/ CAST(\n",
    "        total_quantities_lag AS DECIMAL(16, 5)\n",
    "      ) END AS lag_soe_export_share_ckjr \n",
    "    FROM \n",
    "      aggregate \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          SOE_ownership, \n",
    "          city_prod, \n",
    "          destination,\n",
    "          HS6, \n",
    "          quantities as quantities_lag \n",
    "        FROM \n",
    "          aggregate\n",
    "      ) as lag_quantities ON aggregate.year_lag = lag_quantities.year \n",
    "      AND aggregate.regime = lag_quantities.regime \n",
    "      AND aggregate.SOE_ownership = lag_quantities.SOE_ownership \n",
    "      AND aggregate.city_prod = lag_quantities.city_prod \n",
    "      AND aggregate.HS6 = lag_quantities.HS6 \n",
    "      AND aggregate.destination = lag_quantities.destination \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          destination, \n",
    "          SUM(quantities) as total_quantities_lag \n",
    "        FROM \n",
    "          aggregate \n",
    "        GROUP BY \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          destination\n",
    "      ) as group_lag ON aggregate.year_lag = group_lag.year \n",
    "      AND aggregate.regime = group_lag.regime \n",
    "      AND aggregate.city_prod = group_lag.city_prod \n",
    "      AND aggregate.HS6 = group_lag.HS6 \n",
    "      AND aggregate.destination = group_lag.destination \n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          DISTINCT(citycn) as citycn, \n",
    "          cityen, \n",
    "          geocode4_corr \n",
    "        FROM \n",
    "          chinese_lookup.city_cn_en\n",
    "      ) AS city_cn_en ON city_cn_en.citycn = aggregate.city_prod \n",
    "      LEFT JOIN chinese_lookup.country_cn_en ON country_cn_en.Country_cn = aggregate.destination \n",
    "      WHERE aggregate.SOE_ownership = 'SOE' AND iso_alpha IS NOT NULL\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT CNT, COUNT(*) AS CNT_DUPLICATE\n",
    "FROM (\n",
    "SELECT year_lag, regime, geocode4_corr, iso_alpha, hs6, COUNT(*) as CNT\n",
    "FROM \"chinese_trade\".\"lag_soe_export_ckjr\" \n",
    "GROUP BY \n",
    "year_lag, regime, geocode4_corr, iso_alpha, hs6\n",
    "  )\n",
    "  GROUP BY CNT\n",
    "  ORDER BY CNT_DUPLICATE\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag SOE export share by city, product, regime\n",
    "\n",
    "- Table name: `lag_soe_export_ckr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DROP TABLE lag_soe_export_ckr\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "CREATE TABLE chinese_trade.lag_soe_export_ckr\n",
    "WITH (\n",
    "  format='PARQUET'\n",
    ") AS\n",
    "WITH filter_data AS (\n",
    "  SELECT \n",
    "    date as year, \n",
    "    id, \n",
    "    trade_type, \n",
    "    business_type, \n",
    "    CASE WHEN length(hs) < 5 THEN CONCAT('0', hs) ELSE hs END as hs6, \n",
    "    city_prod,  \n",
    "    quantities, \n",
    "    value, \n",
    "    CASE WHEN trade_type = '进料加工贸易' \n",
    "    OR trade_type = '一般贸易' THEN 'ELIGIBLE' ELSE 'NOT_ELIGIBLE' END as regime, \n",
    "    CASE WHEN Business_type = '国有企业' OR Business_type = '国有' THEN 'SOE' ELSE 'NO_SOE' END as SOE_ownership\n",
    "  FROM \n",
    "    chinese_trade.import_export \n",
    "  WHERE \n",
    "    date in (\n",
    "      '2002', '2003', '2004', '2005', '2006', \n",
    "      '2007', '2008', '2009', '2010'\n",
    "    ) \n",
    "    AND imp_exp = '出口' \n",
    "    AND (\n",
    "      trade_type = '进料加工贸易' \n",
    "      OR trade_type = '一般贸易' \n",
    "      OR trade_type = '来料加工装配贸易' \n",
    "      OR trade_type = '加工贸易'\n",
    "    ) \n",
    "    AND intermediate = 'No' \n",
    "    AND quantities > 0 \n",
    "    AND value > 0\n",
    ") \n",
    "SELECT \n",
    "  * \n",
    "FROM \n",
    "  (\n",
    "    WITH aggregate as (\n",
    "      SELECT \n",
    "        year, \n",
    "        CAST(\n",
    "          CAST(year AS INTEGER) -1 AS VARCHAR\n",
    "        ) as year_lag, \n",
    "        regime, \n",
    "        SOE_ownership, \n",
    "        city_prod, \n",
    "        HS6, \n",
    "        SUM(quantities) as quantities \n",
    "      FROM \n",
    "        filter_data \n",
    "      GROUP BY \n",
    "        year, \n",
    "        regime, \n",
    "        SOE_ownership, \n",
    "        city_prod, \n",
    "        HS6\n",
    "    ) \n",
    "    SELECT \n",
    "      aggregate.year, \n",
    "      aggregate.year_lag, \n",
    "      aggregate.regime, \n",
    "      aggregate.SOE_ownership, \n",
    "      geocode4_corr,\n",
    "      aggregate.HS6, \n",
    "      quantities, \n",
    "      CASE WHEN quantities_lag IS NULL THEN 0 ELSE quantities_lag END AS quantities_lag, \n",
    "      CASE WHEN total_quantities_lag IS NULL THEN 0 ELSE total_quantities_lag END AS total_quantities_lag, \n",
    "      CASE WHEN quantities_lag IS NULL \n",
    "      OR total_quantities_lag IS NULL THEN 0 ELSE CAST(\n",
    "        quantities_lag AS DECIMAL(16, 5)\n",
    "      )/ CAST(\n",
    "        total_quantities_lag AS DECIMAL(16, 5)\n",
    "      ) END AS lag_soe_export_share_ckr\n",
    "    FROM \n",
    "      aggregate \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          SOE_ownership, \n",
    "          city_prod, \n",
    "          HS6, \n",
    "          quantities as quantities_lag \n",
    "        FROM \n",
    "          aggregate\n",
    "      ) as lag_quantities ON aggregate.year_lag = lag_quantities.year \n",
    "      AND aggregate.regime = lag_quantities.regime \n",
    "      AND aggregate.SOE_ownership = lag_quantities.SOE_ownership \n",
    "      AND aggregate.city_prod = lag_quantities.city_prod \n",
    "      AND aggregate.HS6 = lag_quantities.HS6 \n",
    "      LEFT JOIN (\n",
    "        SELECT \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod, \n",
    "          SUM(quantities) as total_quantities_lag \n",
    "        FROM \n",
    "          aggregate \n",
    "        GROUP BY \n",
    "          year, \n",
    "          regime, \n",
    "          HS6, \n",
    "          city_prod \n",
    "      ) as group_lag ON aggregate.year_lag = group_lag.year \n",
    "      AND aggregate.regime = group_lag.regime \n",
    "      AND aggregate.city_prod = group_lag.city_prod \n",
    "      AND aggregate.HS6 = group_lag.HS6\n",
    "      INNER JOIN (\n",
    "        SELECT \n",
    "          DISTINCT(citycn) as citycn, \n",
    "          cityen, \n",
    "          geocode4_corr \n",
    "        FROM \n",
    "          chinese_lookup.city_cn_en\n",
    "      ) AS city_cn_en ON city_cn_en.citycn = aggregate.city_prod \n",
    "      WHERE aggregate.SOE_ownership = 'SOE'\n",
    "  )\n",
    "\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT CNT, COUNT(*) AS CNT_DUPLICATE\n",
    "FROM (\n",
    "SELECT year_lag, regime, geocode4_corr, hs6, COUNT(*) as CNT\n",
    "FROM \"chinese_trade\".\"lag_soe_export_ckr\" \n",
    "GROUP BY \n",
    "year_lag, regime, geocode4_corr, hs6\n",
    "  )\n",
    "  GROUP BY CNT\n",
    "  ORDER BY CNT_DUPLICATE\n",
    "\"\"\"\n",
    "s3.run_query(\n",
    "                    query=query,\n",
    "                    database=db,\n",
    "                    s3_output=s3_output,\n",
    "    filename=\"duplicates\", \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "In this part, we are providing basic summary statistic. Since we have created the tables, we can parse the schema in Glue and use our json file to automatically generates the analysis.\n",
    "\n",
    "The cells below execute the job in the key `ANALYSIS`. You need to change the `primary_key` and `secondary_key` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lag_foreign_export_ckjr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'lag_foreign_export_ckjr'\n",
    "schema = glue.get_table_information(\n",
    "    database = db,\n",
    "    table = table\n",
    ")['Table']\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    db, table\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            db, table, field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "        if field[\"Name\"] != primary_key:\n",
    "            print(\n",
    "                \"Nb of obs for the primary group {} and {}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                )\n",
    "            )\n",
    "            query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"MULTI_PAIR\"].format(\n",
    "                db, table, primary_key, field[\"Name\"]\n",
    "            )\n",
    "\n",
    "            output = s3.run_query(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                s3_output=s3_output,\n",
    "                filename=\"count_categorical_{}_{}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                ),  # Add filename to print dataframe\n",
    "                destination_key=None,  # Add destination key if need to copy output\n",
    "            )\n",
    "\n",
    "            display(\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .assign(total=lambda x: x.sum(axis=1))\n",
    "                                .sort_values(by=[\"total\"])\n",
    "                            ),\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .rename(columns={\"nb_obs\": \"percentage\"})\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .apply(lambda x: x / x.sum(), axis=1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .fillna(0)\n",
    "                    # .sort_index(axis=1, level=1)\n",
    "                    .style.format(\"{0:,.2f}\", subset=[\"nb_obs\", \"total\"])\n",
    "                    .bar(subset=[\"total\"], color=\"#d65f5f\")\n",
    "                    .format(\"{0:,.2%}\", subset=(\"percentage\"))\n",
    "                    .background_gradient(\n",
    "                        cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"nb_obs\")\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "1. Display the percentile\n",
    "2. Display the percentile, with one primary key\n",
    "3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "\n",
    "### 1. Display the percentile\n",
    "\n",
    "- pct: Percentile [.25, .50, .75, .95, .90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "\n",
    "var_index = 0\n",
    "size_continuous = len([len(x) for x in schema[\"StorageDescriptor\"][\"Columns\"] if \n",
    "                       x['Type'] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]])\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(db, table, value[\"Name\"], key)\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"middle_1\"],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"bottom\"].format(\n",
    "                    db, table, value[\"Name\"], key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"middle_2\"\n",
    "            ].format(value[\"Name\"])\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_3\"],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_2\"]\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "(output.sort_values(by=\"pct\").set_index([\"pct\"]).style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "var_index = 0\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(\n",
    "                db, table, value[\"Name\"], key, primary_key\n",
    "            )\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"middle_1\"\n",
    "                ],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"bottom\"\n",
    "                ].format(\n",
    "                    db, table, value[\"Name\"], key, primary_key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\n",
    "                \"ONE_PAIR_DISTRIBUTION\"\n",
    "            ][\"middle_2\"].format(value[\"Name\"], primary_key)\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                        \"top_3\"\n",
    "                    ],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_2\"].format(\n",
    "        primary_key\n",
    "    )\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution_primary_key\",  # Add filename to print dataframe\n",
    "    destination_key=None,  # Add destination key if need to copy output\n",
    ")\n",
    "(\n",
    "    output.set_index([primary_key, \"pct\"])\n",
    "    .unstack(1)\n",
    "    .T.style.format(\"{0:,.2f}\")\n",
    "    .background_gradient(cmap=sns.light_palette(\"blue\", as_cmap=True), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "The primary and secondary key will be passed to all the continuous variables. The output might be too big so we print only the top 10 for the secondary key\n",
    "\n",
    "- index:  Primary group\n",
    "- Columns: \n",
    "    - Secondary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per secondary group value\n",
    "- Heatmap is colored based on the column, ie darker green indicates larger values for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = 'year'\n",
    "secondary_key = 'regime'\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"TWO_PAIRS_DISTRIBUTION\"].format(\n",
    "            db,\n",
    "            table,\n",
    "            primary_key,\n",
    "            secondary_key,\n",
    "            value[\"Name\"],\n",
    "        )\n",
    "\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_distribution_{}_{}_{}\".format(\n",
    "                primary_key, secondary_key, value[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Distribution of {}, by {} and {}\".format(\n",
    "                value[\"Name\"], primary_key, secondary_key,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.loc[\n",
    "                    lambda x: x[secondary_key].isin(\n",
    "                        (\n",
    "                            output.assign(\n",
    "                                total_secondary=lambda x: x[value[\"Name\"]]\n",
    "                                .groupby([x[secondary_key]])\n",
    "                                .transform(\"sum\")\n",
    "                            )\n",
    "                            .drop_duplicates(subset=\"total_secondary\", keep=\"last\")\n",
    "                            .sort_values(by=[\"total_secondary\"], ascending=False)\n",
    "                            .iloc[:10, 1]\n",
    "                        ).to_list()\n",
    "                    )\n",
    "                ]\n",
    "                .set_index([primary_key, \"pct\", secondary_key])\n",
    "                .unstack([0, 1])\n",
    "                .fillna(0)\n",
    "                .sort_index(axis=1, level=[1, 2])\n",
    "                .style.format(\"{0:,.2f}\")\n",
    "                .background_gradient(cmap=sns.light_palette(\"green\", as_cmap=True))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lag_foreign_export_ckr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'lag_foreign_export_ckr'\n",
    "schema = glue.get_table_information(\n",
    "    database = db,\n",
    "    table = table\n",
    ")['Table']\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    db, table\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            db, table, field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "        if field[\"Name\"] != primary_key:\n",
    "            print(\n",
    "                \"Nb of obs for the primary group {} and {}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                )\n",
    "            )\n",
    "            query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"MULTI_PAIR\"].format(\n",
    "                db, table, primary_key, field[\"Name\"]\n",
    "            )\n",
    "\n",
    "            output = s3.run_query(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                s3_output=s3_output,\n",
    "                filename=\"count_categorical_{}_{}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                ),  # Add filename to print dataframe\n",
    "                destination_key=None,  # Add destination key if need to copy output\n",
    "            )\n",
    "\n",
    "            display(\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .assign(total=lambda x: x.sum(axis=1))\n",
    "                                .sort_values(by=[\"total\"])\n",
    "                            ),\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .rename(columns={\"nb_obs\": \"percentage\"})\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .apply(lambda x: x / x.sum(), axis=1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .fillna(0)\n",
    "                    # .sort_index(axis=1, level=1)\n",
    "                    .style.format(\"{0:,.2f}\", subset=[\"nb_obs\", \"total\"])\n",
    "                    .bar(subset=[\"total\"], color=\"#d65f5f\")\n",
    "                    .format(\"{0:,.2%}\", subset=(\"percentage\"))\n",
    "                    .background_gradient(\n",
    "                        cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"nb_obs\")\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "1. Display the percentile\n",
    "2. Display the percentile, with one primary key\n",
    "3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "\n",
    "### 1. Display the percentile\n",
    "\n",
    "- pct: Percentile [.25, .50, .75, .95, .90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "\n",
    "var_index = 0\n",
    "size_continuous = len([len(x) for x in schema[\"StorageDescriptor\"][\"Columns\"] if \n",
    "                       x['Type'] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]])\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(db, table, value[\"Name\"], key)\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"middle_1\"],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"bottom\"].format(\n",
    "                    db, table, value[\"Name\"], key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"middle_2\"\n",
    "            ].format(value[\"Name\"])\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_3\"],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_2\"]\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "(output.sort_values(by=\"pct\").set_index([\"pct\"]).style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "var_index = 0\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(\n",
    "                db, table, value[\"Name\"], key, primary_key\n",
    "            )\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"middle_1\"\n",
    "                ],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"bottom\"\n",
    "                ].format(\n",
    "                    db, table, value[\"Name\"], key, primary_key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\n",
    "                \"ONE_PAIR_DISTRIBUTION\"\n",
    "            ][\"middle_2\"].format(value[\"Name\"], primary_key)\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                        \"top_3\"\n",
    "                    ],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_2\"].format(\n",
    "        primary_key\n",
    "    )\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution_primary_key\",  # Add filename to print dataframe\n",
    "    destination_key=None,  # Add destination key if need to copy output\n",
    ")\n",
    "(\n",
    "    output.set_index([primary_key, \"pct\"])\n",
    "    .unstack(1)\n",
    "    .T.style.format(\"{0:,.2f}\")\n",
    "    .background_gradient(cmap=sns.light_palette(\"blue\", as_cmap=True), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "The primary and secondary key will be passed to all the continuous variables. The output might be too big so we print only the top 10 for the secondary key\n",
    "\n",
    "- index:  Primary group\n",
    "- Columns: \n",
    "    - Secondary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per secondary group value\n",
    "- Heatmap is colored based on the column, ie darker green indicates larger values for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = 'year'\n",
    "secondary_key = 'regime'\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"TWO_PAIRS_DISTRIBUTION\"].format(\n",
    "            db,\n",
    "            table,\n",
    "            primary_key,\n",
    "            secondary_key,\n",
    "            value[\"Name\"],\n",
    "        )\n",
    "\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_distribution_{}_{}_{}\".format(\n",
    "                primary_key, secondary_key, value[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Distribution of {}, by {} and {}\".format(\n",
    "                value[\"Name\"], primary_key, secondary_key,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.loc[\n",
    "                    lambda x: x[secondary_key].isin(\n",
    "                        (\n",
    "                            output.assign(\n",
    "                                total_secondary=lambda x: x[value[\"Name\"]]\n",
    "                                .groupby([x[secondary_key]])\n",
    "                                .transform(\"sum\")\n",
    "                            )\n",
    "                            .drop_duplicates(subset=\"total_secondary\", keep=\"last\")\n",
    "                            .sort_values(by=[\"total_secondary\"], ascending=False)\n",
    "                            .iloc[:10, 1]\n",
    "                        ).to_list()\n",
    "                    )\n",
    "                ]\n",
    "                .set_index([primary_key, \"pct\", secondary_key])\n",
    "                .unstack([0, 1])\n",
    "                .fillna(0)\n",
    "                .sort_index(axis=1, level=[1, 2])\n",
    "                .style.format(\"{0:,.2f}\")\n",
    "                .background_gradient(cmap=sns.light_palette(\"green\", as_cmap=True))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lag_soe_export_ckjr` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'lag_soe_export_ckjr'\n",
    "schema = glue.get_table_information(\n",
    "    database = db,\n",
    "    table = table\n",
    ")['Table']\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    db, table\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            db, table, field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "        if field[\"Name\"] != primary_key:\n",
    "            print(\n",
    "                \"Nb of obs for the primary group {} and {}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                )\n",
    "            )\n",
    "            query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"MULTI_PAIR\"].format(\n",
    "                db, table, primary_key, field[\"Name\"]\n",
    "            )\n",
    "\n",
    "            output = s3.run_query(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                s3_output=s3_output,\n",
    "                filename=\"count_categorical_{}_{}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                ),  # Add filename to print dataframe\n",
    "                destination_key=None,  # Add destination key if need to copy output\n",
    "            )\n",
    "\n",
    "            display(\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .assign(total=lambda x: x.sum(axis=1))\n",
    "                                .sort_values(by=[\"total\"])\n",
    "                            ),\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .rename(columns={\"nb_obs\": \"percentage\"})\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .apply(lambda x: x / x.sum(), axis=1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .fillna(0)\n",
    "                    # .sort_index(axis=1, level=1)\n",
    "                    .style.format(\"{0:,.2f}\", subset=[\"nb_obs\", \"total\"])\n",
    "                    .bar(subset=[\"total\"], color=\"#d65f5f\")\n",
    "                    .format(\"{0:,.2%}\", subset=(\"percentage\"))\n",
    "                    .background_gradient(\n",
    "                        cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"nb_obs\")\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "1. Display the percentile\n",
    "2. Display the percentile, with one primary key\n",
    "3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "\n",
    "### 1. Display the percentile\n",
    "\n",
    "- pct: Percentile [.25, .50, .75, .95, .90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "\n",
    "var_index = 0\n",
    "size_continuous = len([len(x) for x in schema[\"StorageDescriptor\"][\"Columns\"] if \n",
    "                       x['Type'] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]])\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(db, table, value[\"Name\"], key)\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"middle_1\"],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"bottom\"].format(\n",
    "                    db, table, value[\"Name\"], key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"middle_2\"\n",
    "            ].format(value[\"Name\"])\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_3\"],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_2\"]\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "(output.sort_values(by=\"pct\").set_index([\"pct\"]).style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "var_index = 0\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(\n",
    "                db, table, value[\"Name\"], key, primary_key\n",
    "            )\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"middle_1\"\n",
    "                ],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"bottom\"\n",
    "                ].format(\n",
    "                    db, table, value[\"Name\"], key, primary_key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\n",
    "                \"ONE_PAIR_DISTRIBUTION\"\n",
    "            ][\"middle_2\"].format(value[\"Name\"], primary_key)\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                        \"top_3\"\n",
    "                    ],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_2\"].format(\n",
    "        primary_key\n",
    "    )\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution_primary_key\",  # Add filename to print dataframe\n",
    "    destination_key=None,  # Add destination key if need to copy output\n",
    ")\n",
    "(\n",
    "    output.set_index([primary_key, \"pct\"])\n",
    "    .unstack(1)\n",
    "    .T.style.format(\"{0:,.2f}\")\n",
    "    .background_gradient(cmap=sns.light_palette(\"blue\", as_cmap=True), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "The primary and secondary key will be passed to all the continuous variables. The output might be too big so we print only the top 10 for the secondary key\n",
    "\n",
    "- index:  Primary group\n",
    "- Columns: \n",
    "    - Secondary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per secondary group value\n",
    "- Heatmap is colored based on the column, ie darker green indicates larger values for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = 'year'\n",
    "secondary_key = 'regime'\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"TWO_PAIRS_DISTRIBUTION\"].format(\n",
    "            db,\n",
    "            table,\n",
    "            primary_key,\n",
    "            secondary_key,\n",
    "            value[\"Name\"],\n",
    "        )\n",
    "\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_distribution_{}_{}_{}\".format(\n",
    "                primary_key, secondary_key, value[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Distribution of {}, by {} and {}\".format(\n",
    "                value[\"Name\"], primary_key, secondary_key,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.loc[\n",
    "                    lambda x: x[secondary_key].isin(\n",
    "                        (\n",
    "                            output.assign(\n",
    "                                total_secondary=lambda x: x[value[\"Name\"]]\n",
    "                                .groupby([x[secondary_key]])\n",
    "                                .transform(\"sum\")\n",
    "                            )\n",
    "                            .drop_duplicates(subset=\"total_secondary\", keep=\"last\")\n",
    "                            .sort_values(by=[\"total_secondary\"], ascending=False)\n",
    "                            .iloc[:10, 1]\n",
    "                        ).to_list()\n",
    "                    )\n",
    "                ]\n",
    "                .set_index([primary_key, \"pct\", secondary_key])\n",
    "                .unstack([0, 1])\n",
    "                .fillna(0)\n",
    "                .sort_index(axis=1, level=[1, 2])\n",
    "                .style.format(\"{0:,.2f}\")\n",
    "                .background_gradient(cmap=sns.light_palette(\"green\", as_cmap=True))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lag_soe_export_ckr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'lag_soe_export_ckr'\n",
    "schema = glue.get_table_information(\n",
    "    database = db,\n",
    "    table = table\n",
    ")['Table']\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"top\"]\n",
    "table_middle = \"\"\n",
    "table_bottom = parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"bottom\"].format(\n",
    "    db, table\n",
    ")\n",
    "\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if key == len(schema[\"StorageDescriptor\"][\"Columns\"]) - 1:\n",
    "\n",
    "        table_middle += \"{} \".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "    else:\n",
    "        table_middle += \"{} ,\".format(\n",
    "            parameters[\"ANALYSIS\"][\"COUNT_MISSING\"][\"middle\"].format(value[\"Name\"])\n",
    "        )\n",
    "query = table_top + table_middle + table_bottom\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_missing\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "display(\n",
    "    output.T.rename(columns={0: \"total_missing\"})\n",
    "    .assign(total_missing_pct=lambda x: x[\"total_missing\"] / x.iloc[0, 0])\n",
    "    .sort_values(by=[\"total_missing\"], ascending=False)\n",
    "    .style.format(\"{0:,.2%}\", subset=[\"total_missing_pct\"])\n",
    "    .bar(subset=\"total_missing_pct\", color=[\"#d65f5f\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "### Count obs by group\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "\n",
    "        print(\"Nb of obs for {}\".format(field[\"Name\"]))\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"PAIR\"].format(\n",
    "            db, table, field[\"Name\"]\n",
    "        )\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_categorical_{}\".format(\n",
    "                field[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        ### Print top 10\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.set_index([field[\"Name\"]])\n",
    "                .assign(percentage=lambda x: x[\"nb_obs\"] / x[\"nb_obs\"].sum())\n",
    "                .sort_values(\"percentage\", ascending=False)\n",
    "                .head(10)\n",
    "                .style.format(\"{0:.2%}\", subset=[\"percentage\"])\n",
    "                .bar(subset=[\"percentage\"], color=\"#d65f5f\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "for field in schema[\"StorageDescriptor\"][\"Columns\"]:\n",
    "    if field[\"Type\"] in [\"string\", \"object\", \"varchar(10)\",\"varchar(12)\"]:\n",
    "        if field[\"Name\"] != primary_key:\n",
    "            print(\n",
    "                \"Nb of obs for the primary group {} and {}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                )\n",
    "            )\n",
    "            query = parameters[\"ANALYSIS\"][\"CATEGORICAL\"][\"MULTI_PAIR\"].format(\n",
    "                db, table, primary_key, field[\"Name\"]\n",
    "            )\n",
    "\n",
    "            output = s3.run_query(\n",
    "                query=query,\n",
    "                database=db,\n",
    "                s3_output=s3_output,\n",
    "                filename=\"count_categorical_{}_{}\".format(\n",
    "                    primary_key, field[\"Name\"]\n",
    "                ),  # Add filename to print dataframe\n",
    "                destination_key=None,  # Add destination key if need to copy output\n",
    "            )\n",
    "\n",
    "            display(\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .assign(total=lambda x: x.sum(axis=1))\n",
    "                                .sort_values(by=[\"total\"])\n",
    "                            ),\n",
    "                            (\n",
    "                                output.loc[\n",
    "                                    lambda x: x[field[\"Name\"]].isin(\n",
    "                                        (\n",
    "                                            output.assign(\n",
    "                                                total_secondary=lambda x: x[\"nb_obs\"]\n",
    "                                                .groupby([x[field[\"Name\"]]])\n",
    "                                                .transform(\"sum\")\n",
    "                                            )\n",
    "                                            .drop_duplicates(\n",
    "                                                subset=\"total_secondary\", keep=\"last\"\n",
    "                                            )\n",
    "                                            .sort_values(\n",
    "                                                by=[\"total_secondary\"], ascending=False\n",
    "                                            )\n",
    "                                            .iloc[:10, 1]\n",
    "                                            .to_list()\n",
    "                                        )\n",
    "                                    )\n",
    "                                ]\n",
    "                                .rename(columns={\"nb_obs\": \"percentage\"})\n",
    "                                .set_index([primary_key, field[\"Name\"]])\n",
    "                                .unstack([0])\n",
    "                                .fillna(0)\n",
    "                                .apply(lambda x: x / x.sum(), axis=1)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .fillna(0)\n",
    "                    # .sort_index(axis=1, level=1)\n",
    "                    .style.format(\"{0:,.2f}\", subset=[\"nb_obs\", \"total\"])\n",
    "                    .bar(subset=[\"total\"], color=\"#d65f5f\")\n",
    "                    .format(\"{0:,.2%}\", subset=(\"percentage\"))\n",
    "                    .background_gradient(\n",
    "                        cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"nb_obs\")\n",
    "                    )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "1. Display the percentile\n",
    "2. Display the percentile, with one primary key\n",
    "3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "\n",
    "### 1. Display the percentile\n",
    "\n",
    "- pct: Percentile [.25, .50, .75, .95, .90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "\n",
    "var_index = 0\n",
    "size_continuous = len([len(x) for x in schema[\"StorageDescriptor\"][\"Columns\"] if \n",
    "                       x['Type'] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]])\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(db, table, value[\"Name\"], key)\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"middle_1\"],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"bottom\"].format(\n",
    "                    db, table, value[\"Name\"], key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\n",
    "                \"middle_2\"\n",
    "            ].format(value[\"Name\"])\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_3\"],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"DISTRIBUTION\"][\"top_2\"]\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution\",  ## Add filename to print dataframe\n",
    "    destination_key=None,  ### Add destination key if need to copy output\n",
    ")\n",
    "(output.sort_values(by=\"pct\").set_index([\"pct\"]).style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"year\"\n",
    "table_top = \"\"\n",
    "table_top_var = \"\"\n",
    "table_middle = \"\"\n",
    "table_bottom = \"\"\n",
    "var_index = 0\n",
    "cont = 0\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "        cont +=1\n",
    "\n",
    "        if var_index == 0:\n",
    "            table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "            table_top = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                \"bottom\"\n",
    "            ].format(\n",
    "                db, table, value[\"Name\"], key, primary_key\n",
    "            )\n",
    "        else:\n",
    "            temp_middle_1 = \"{} {}\".format(\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"middle_1\"\n",
    "                ],\n",
    "                parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                    \"bottom\"\n",
    "                ].format(\n",
    "                    db, table, value[\"Name\"], key, primary_key\n",
    "                ),\n",
    "            )\n",
    "            temp_middle_2 = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\n",
    "                \"ONE_PAIR_DISTRIBUTION\"\n",
    "            ][\"middle_2\"].format(value[\"Name\"], primary_key)\n",
    "\n",
    "            if cont == size_continuous:\n",
    "\n",
    "                table_top_var += \"{} {}\".format(\n",
    "                    value[\"Name\"],\n",
    "                    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\n",
    "                        \"top_3\"\n",
    "                    ],\n",
    "                )\n",
    "                table_bottom += \"{} {})\".format(temp_middle_1, temp_middle_2)\n",
    "            else:\n",
    "                table_top_var += \"{} ,\".format(value[\"Name\"])\n",
    "                table_bottom += \"{} {}\".format(temp_middle_1, temp_middle_2)\n",
    "        var_index += 1\n",
    "\n",
    "query = (\n",
    "    parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_1\"]\n",
    "    + table_top\n",
    "    + parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"ONE_PAIR_DISTRIBUTION\"][\"top_2\"].format(\n",
    "        primary_key\n",
    "    )\n",
    "    + table_top_var\n",
    "    + table_bottom\n",
    ")\n",
    "output = s3.run_query(\n",
    "    query=query,\n",
    "    database=db,\n",
    "    s3_output=s3_output,\n",
    "    filename=\"count_distribution_primary_key\",  # Add filename to print dataframe\n",
    "    destination_key=None,  # Add destination key if need to copy output\n",
    ")\n",
    "(\n",
    "    output.set_index([primary_key, \"pct\"])\n",
    "    .unstack(1)\n",
    "    .T.style.format(\"{0:,.2f}\")\n",
    "    .background_gradient(cmap=sns.light_palette(\"blue\", as_cmap=True), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Display the percentile, with one primary key, and a secondary key\n",
    "\n",
    "The primary and secondary key will be passed to all the continuous variables. The output might be too big so we print only the top 10 for the secondary key\n",
    "\n",
    "- index:  Primary group\n",
    "- Columns: \n",
    "    - Secondary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per secondary group value\n",
    "- Heatmap is colored based on the column, ie darker green indicates larger values for a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = 'year'\n",
    "secondary_key = 'regime'\n",
    "for key, value in enumerate(schema[\"StorageDescriptor\"][\"Columns\"]):\n",
    "\n",
    "    if value[\"Type\"] in [\"float\", \"double\", \"bigint\", \"decimal(21,5)\"]:\n",
    "\n",
    "        query = parameters[\"ANALYSIS\"][\"CONTINUOUS\"][\"TWO_PAIRS_DISTRIBUTION\"].format(\n",
    "            db,\n",
    "            table,\n",
    "            primary_key,\n",
    "            secondary_key,\n",
    "            value[\"Name\"],\n",
    "        )\n",
    "\n",
    "        output = s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=s3_output,\n",
    "            filename=\"count_distribution_{}_{}_{}\".format(\n",
    "                primary_key, secondary_key, value[\"Name\"]\n",
    "            ),  ## Add filename to print dataframe\n",
    "            destination_key=None,  ### Add destination key if need to copy output\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Distribution of {}, by {} and {}\".format(\n",
    "                value[\"Name\"], primary_key, secondary_key,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        display(\n",
    "            (\n",
    "                output.loc[\n",
    "                    lambda x: x[secondary_key].isin(\n",
    "                        (\n",
    "                            output.assign(\n",
    "                                total_secondary=lambda x: x[value[\"Name\"]]\n",
    "                                .groupby([x[secondary_key]])\n",
    "                                .transform(\"sum\")\n",
    "                            )\n",
    "                            .drop_duplicates(subset=\"total_secondary\", keep=\"last\")\n",
    "                            .sort_values(by=[\"total_secondary\"], ascending=False)\n",
    "                            .iloc[:10, 1]\n",
    "                        ).to_list()\n",
    "                    )\n",
    "                ]\n",
    "                .set_index([primary_key, \"pct\", secondary_key])\n",
    "                .unstack([0, 1])\n",
    "                .fillna(0)\n",
    "                .sort_index(axis=1, level=[1, 2])\n",
    "                .style.format(\"{0:,.2f}\")\n",
    "                .background_gradient(cmap=sns.light_palette(\"green\", as_cmap=True))\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.22.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
